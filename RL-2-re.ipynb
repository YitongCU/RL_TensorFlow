{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-06 22:55:35,412] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "H=10\n",
    "batch_size = 5\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # more focus on future reward\n",
    "\n",
    "D = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32,shape= [None,D], name = \"input_x\")\n",
    "W1 = tf.get_variable('W1',shape = [D,H], initializer = tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable('W2',shape = [H,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "probability = tf.nn.sigmoid(tf.matmul(layer1,W2))\n",
    "\n",
    "tvars = tf.trainable_variables() ## store variabes W1 W2, list, list[0] W1 shape:[4,10] \n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\") ## ??\n",
    "\n",
    "loglik = tf.log(input_y*(input_y-probability)+(1-input_y)*(input_y+probability))\n",
    "loss = -tf.reduce_mean(loglik*advantages)\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name = 'batch_grad1')\n",
    "W2Grad = tf.placeholder(tf.float32,name = 'batch_grad2')\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0,len(r))):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 21.600000.  Total average reward 21.600000.\n",
      "Average reward for episode 31.000000.  Total average reward 21.694000.\n",
      "Average reward for episode 31.000000.  Total average reward 21.787060.\n",
      "Average reward for episode 27.200000.  Total average reward 21.841189.\n",
      "Average reward for episode 31.600000.  Total average reward 21.938778.\n",
      "Average reward for episode 29.200000.  Total average reward 22.011390.\n",
      "Average reward for episode 30.400000.  Total average reward 22.095276.\n",
      "Average reward for episode 19.200000.  Total average reward 22.066323.\n",
      "Average reward for episode 20.600000.  Total average reward 22.051660.\n",
      "Average reward for episode 22.200000.  Total average reward 22.053143.\n",
      "Average reward for episode 33.800000.  Total average reward 22.170612.\n",
      "Average reward for episode 22.200000.  Total average reward 22.170906.\n",
      "Average reward for episode 34.000000.  Total average reward 22.289197.\n",
      "Average reward for episode 49.000000.  Total average reward 22.556305.\n",
      "Average reward for episode 22.800000.  Total average reward 22.558742.\n",
      "Average reward for episode 31.800000.  Total average reward 22.651154.\n",
      "Average reward for episode 21.600000.  Total average reward 22.640643.\n",
      "Average reward for episode 28.400000.  Total average reward 22.698236.\n",
      "Average reward for episode 19.600000.  Total average reward 22.667254.\n",
      "Average reward for episode 17.200000.  Total average reward 22.612581.\n",
      "Average reward for episode 30.400000.  Total average reward 22.690456.\n",
      "Average reward for episode 24.400000.  Total average reward 22.707551.\n",
      "Average reward for episode 26.800000.  Total average reward 22.748475.\n",
      "Average reward for episode 21.800000.  Total average reward 22.738991.\n",
      "Average reward for episode 16.200000.  Total average reward 22.673601.\n",
      "Average reward for episode 32.800000.  Total average reward 22.774865.\n",
      "Average reward for episode 19.000000.  Total average reward 22.737116.\n",
      "Average reward for episode 15.800000.  Total average reward 22.667745.\n",
      "Average reward for episode 35.200000.  Total average reward 22.793068.\n",
      "Average reward for episode 23.200000.  Total average reward 22.797137.\n",
      "Average reward for episode 15.600000.  Total average reward 22.725165.\n",
      "Average reward for episode 25.600000.  Total average reward 22.753914.\n",
      "Average reward for episode 22.000000.  Total average reward 22.746375.\n",
      "Average reward for episode 22.400000.  Total average reward 22.742911.\n",
      "Average reward for episode 25.200000.  Total average reward 22.767482.\n",
      "Average reward for episode 27.200000.  Total average reward 22.811807.\n",
      "Average reward for episode 39.000000.  Total average reward 22.973689.\n",
      "Average reward for episode 27.000000.  Total average reward 23.013952.\n",
      "Average reward for episode 26.600000.  Total average reward 23.049813.\n",
      "Average reward for episode 26.200000.  Total average reward 23.081314.\n",
      "Average reward for episode 22.400000.  Total average reward 23.074501.\n",
      "Average reward for episode 26.000000.  Total average reward 23.103756.\n",
      "Average reward for episode 39.800000.  Total average reward 23.270719.\n",
      "Average reward for episode 27.000000.  Total average reward 23.308012.\n",
      "Average reward for episode 28.800000.  Total average reward 23.362931.\n",
      "Average reward for episode 21.800000.  Total average reward 23.347302.\n",
      "Average reward for episode 26.600000.  Total average reward 23.379829.\n",
      "Average reward for episode 32.600000.  Total average reward 23.472031.\n",
      "Average reward for episode 23.800000.  Total average reward 23.475310.\n",
      "Average reward for episode 24.000000.  Total average reward 23.480557.\n",
      "Average reward for episode 29.000000.  Total average reward 23.535752.\n",
      "Average reward for episode 30.400000.  Total average reward 23.604394.\n",
      "Average reward for episode 32.000000.  Total average reward 23.688350.\n",
      "Average reward for episode 23.200000.  Total average reward 23.683467.\n",
      "Average reward for episode 24.400000.  Total average reward 23.690632.\n",
      "Average reward for episode 25.800000.  Total average reward 23.711726.\n",
      "Average reward for episode 14.800000.  Total average reward 23.622609.\n",
      "Average reward for episode 30.800000.  Total average reward 23.694382.\n",
      "Average reward for episode 18.400000.  Total average reward 23.641439.\n",
      "Average reward for episode 20.200000.  Total average reward 23.607024.\n",
      "Average reward for episode 22.200000.  Total average reward 23.592954.\n",
      "Average reward for episode 24.600000.  Total average reward 23.603024.\n",
      "Average reward for episode 36.200000.  Total average reward 23.728994.\n",
      "Average reward for episode 28.800000.  Total average reward 23.779704.\n",
      "Average reward for episode 22.400000.  Total average reward 23.765907.\n",
      "Average reward for episode 23.800000.  Total average reward 23.766248.\n",
      "Average reward for episode 28.200000.  Total average reward 23.810586.\n",
      "Average reward for episode 28.400000.  Total average reward 23.856480.\n",
      "Average reward for episode 27.400000.  Total average reward 23.891915.\n",
      "Average reward for episode 20.600000.  Total average reward 23.858996.\n",
      "Average reward for episode 29.600000.  Total average reward 23.916406.\n",
      "Average reward for episode 21.400000.  Total average reward 23.891242.\n",
      "Average reward for episode 23.600000.  Total average reward 23.888329.\n",
      "Average reward for episode 22.200000.  Total average reward 23.871446.\n",
      "Average reward for episode 33.200000.  Total average reward 23.964732.\n",
      "Average reward for episode 19.400000.  Total average reward 23.919084.\n",
      "Average reward for episode 19.200000.  Total average reward 23.871894.\n",
      "Average reward for episode 34.600000.  Total average reward 23.979175.\n",
      "Average reward for episode 28.400000.  Total average reward 24.023383.\n",
      "Average reward for episode 24.400000.  Total average reward 24.027149.\n",
      "Average reward for episode 28.200000.  Total average reward 24.068878.\n",
      "Average reward for episode 23.200000.  Total average reward 24.060189.\n",
      "Average reward for episode 22.800000.  Total average reward 24.047587.\n",
      "Average reward for episode 27.200000.  Total average reward 24.079111.\n",
      "Average reward for episode 19.800000.  Total average reward 24.036320.\n",
      "Average reward for episode 27.800000.  Total average reward 24.073957.\n",
      "Average reward for episode 25.800000.  Total average reward 24.091217.\n",
      "Average reward for episode 21.800000.  Total average reward 24.068305.\n",
      "Average reward for episode 20.800000.  Total average reward 24.035622.\n",
      "Average reward for episode 15.000000.  Total average reward 23.945266.\n",
      "Average reward for episode 30.400000.  Total average reward 24.009813.\n",
      "Average reward for episode 17.200000.  Total average reward 23.941715.\n",
      "Average reward for episode 19.200000.  Total average reward 23.894298.\n",
      "Average reward for episode 22.200000.  Total average reward 23.877355.\n",
      "Average reward for episode 16.200000.  Total average reward 23.800581.\n",
      "Average reward for episode 26.400000.  Total average reward 23.826575.\n",
      "Average reward for episode 21.200000.  Total average reward 23.800310.\n",
      "Average reward for episode 21.600000.  Total average reward 23.778307.\n",
      "Average reward for episode 23.000000.  Total average reward 23.770524.\n",
      "Average reward for episode 28.600000.  Total average reward 23.818818.\n",
      "Average reward for episode 58.000000.  Total average reward 24.160630.\n",
      "Average reward for episode 34.800000.  Total average reward 24.267024.\n",
      "Average reward for episode 25.000000.  Total average reward 24.274354.\n",
      "Average reward for episode 25.600000.  Total average reward 24.287610.\n",
      "Average reward for episode 21.000000.  Total average reward 24.254734.\n",
      "Average reward for episode 26.400000.  Total average reward 24.276187.\n",
      "Average reward for episode 22.800000.  Total average reward 24.261425.\n",
      "Average reward for episode 17.200000.  Total average reward 24.190810.\n",
      "Average reward for episode 25.600000.  Total average reward 24.204902.\n",
      "Average reward for episode 21.800000.  Total average reward 24.180853.\n",
      "Average reward for episode 15.600000.  Total average reward 24.095045.\n",
      "Average reward for episode 22.000000.  Total average reward 24.074094.\n",
      "Average reward for episode 24.600000.  Total average reward 24.079353.\n",
      "Average reward for episode 23.200000.  Total average reward 24.070560.\n",
      "Average reward for episode 20.600000.  Total average reward 24.035854.\n",
      "Average reward for episode 22.200000.  Total average reward 24.017496.\n",
      "Average reward for episode 30.800000.  Total average reward 24.085321.\n",
      "Average reward for episode 31.400000.  Total average reward 24.158468.\n",
      "Average reward for episode 31.400000.  Total average reward 24.230883.\n",
      "Average reward for episode 23.600000.  Total average reward 24.224574.\n",
      "Average reward for episode 31.000000.  Total average reward 24.292328.\n",
      "Average reward for episode 17.800000.  Total average reward 24.227405.\n",
      "Average reward for episode 31.600000.  Total average reward 24.301131.\n",
      "Average reward for episode 17.600000.  Total average reward 24.234120.\n",
      "Average reward for episode 26.800000.  Total average reward 24.259778.\n",
      "Average reward for episode 19.800000.  Total average reward 24.215181.\n",
      "Average reward for episode 20.800000.  Total average reward 24.181029.\n",
      "Average reward for episode 34.200000.  Total average reward 24.281219.\n",
      "Average reward for episode 30.600000.  Total average reward 24.344406.\n",
      "Average reward for episode 23.400000.  Total average reward 24.334962.\n",
      "Average reward for episode 20.400000.  Total average reward 24.295613.\n",
      "Average reward for episode 24.200000.  Total average reward 24.294657.\n",
      "Average reward for episode 17.400000.  Total average reward 24.225710.\n",
      "Average reward for episode 38.600000.  Total average reward 24.369453.\n",
      "Average reward for episode 20.400000.  Total average reward 24.329758.\n",
      "Average reward for episode 33.400000.  Total average reward 24.420461.\n",
      "Average reward for episode 21.200000.  Total average reward 24.388256.\n",
      "Average reward for episode 25.400000.  Total average reward 24.398374.\n",
      "Average reward for episode 24.600000.  Total average reward 24.400390.\n",
      "Average reward for episode 24.600000.  Total average reward 24.402386.\n",
      "Average reward for episode 22.000000.  Total average reward 24.378362.\n",
      "Average reward for episode 34.800000.  Total average reward 24.482579.\n",
      "Average reward for episode 23.600000.  Total average reward 24.473753.\n",
      "Average reward for episode 27.600000.  Total average reward 24.505015.\n",
      "Average reward for episode 15.800000.  Total average reward 24.417965.\n",
      "Average reward for episode 26.800000.  Total average reward 24.441785.\n",
      "Average reward for episode 34.000000.  Total average reward 24.537368.\n",
      "Average reward for episode 24.400000.  Total average reward 24.535994.\n",
      "Average reward for episode 20.400000.  Total average reward 24.494634.\n",
      "Average reward for episode 25.200000.  Total average reward 24.501688.\n",
      "Average reward for episode 38.000000.  Total average reward 24.636671.\n",
      "Average reward for episode 27.400000.  Total average reward 24.664304.\n",
      "Average reward for episode 16.800000.  Total average reward 24.585661.\n",
      "Average reward for episode 20.200000.  Total average reward 24.541804.\n",
      "Average reward for episode 20.000000.  Total average reward 24.496386.\n",
      "Average reward for episode 21.200000.  Total average reward 24.463422.\n",
      "Average reward for episode 23.200000.  Total average reward 24.450788.\n",
      "Average reward for episode 29.600000.  Total average reward 24.502280.\n",
      "Average reward for episode 16.800000.  Total average reward 24.425258.\n",
      "Average reward for episode 34.600000.  Total average reward 24.527005.\n",
      "Average reward for episode 25.200000.  Total average reward 24.533735.\n",
      "Average reward for episode 23.000000.  Total average reward 24.518398.\n",
      "Average reward for episode 24.200000.  Total average reward 24.515214.\n",
      "Average reward for episode 32.200000.  Total average reward 24.592061.\n",
      "Average reward for episode 25.800000.  Total average reward 24.604141.\n",
      "Average reward for episode 22.600000.  Total average reward 24.584099.\n",
      "Average reward for episode 27.400000.  Total average reward 24.612258.\n",
      "Average reward for episode 17.200000.  Total average reward 24.538136.\n",
      "Average reward for episode 26.400000.  Total average reward 24.556755.\n",
      "Average reward for episode 26.800000.  Total average reward 24.579187.\n",
      "Average reward for episode 31.800000.  Total average reward 24.651395.\n",
      "Average reward for episode 30.200000.  Total average reward 24.706881.\n",
      "Average reward for episode 21.600000.  Total average reward 24.675812.\n",
      "Average reward for episode 29.000000.  Total average reward 24.719054.\n",
      "Average reward for episode 25.400000.  Total average reward 24.725864.\n",
      "Average reward for episode 26.600000.  Total average reward 24.744605.\n",
      "Average reward for episode 38.200000.  Total average reward 24.879159.\n",
      "Average reward for episode 25.600000.  Total average reward 24.886367.\n",
      "Average reward for episode 19.600000.  Total average reward 24.833504.\n",
      "Average reward for episode 24.000000.  Total average reward 24.825169.\n",
      "Average reward for episode 26.000000.  Total average reward 24.836917.\n",
      "Average reward for episode 26.600000.  Total average reward 24.854548.\n",
      "Average reward for episode 14.600000.  Total average reward 24.752002.\n",
      "Average reward for episode 27.600000.  Total average reward 24.780482.\n",
      "Average reward for episode 33.400000.  Total average reward 24.866677.\n",
      "Average reward for episode 39.000000.  Total average reward 25.008011.\n",
      "Average reward for episode 27.800000.  Total average reward 25.035931.\n",
      "Average reward for episode 21.400000.  Total average reward 24.999571.\n",
      "Average reward for episode 20.400000.  Total average reward 24.953576.\n",
      "Average reward for episode 17.000000.  Total average reward 24.874040.\n",
      "Average reward for episode 22.200000.  Total average reward 24.847299.\n",
      "Average reward for episode 25.400000.  Total average reward 24.852826.\n",
      "Average reward for episode 16.800000.  Total average reward 24.772298.\n",
      "Average reward for episode 32.400000.  Total average reward 24.848575.\n",
      "Average reward for episode 24.800000.  Total average reward 24.848089.\n",
      "Average reward for episode 19.800000.  Total average reward 24.797609.\n",
      "Average reward for episode 25.600000.  Total average reward 24.805632.\n",
      "Average reward for episode 29.800000.  Total average reward 24.855576.\n",
      "Average reward for episode 17.200000.  Total average reward 24.779020.\n",
      "Average reward for episode 28.200000.  Total average reward 24.813230.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    rendering  = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    \n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad*0\n",
    "    \n",
    "    while episode_number<=1000:\n",
    "        \n",
    "        if reward_sum/batch_size > 100 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "        \n",
    "        x = np.reshape(observation,[1,D])\n",
    "        tfprob = sess.run(probability,feed_dict={observations:x})\n",
    "        \n",
    "        action = 1 if np.random.uniform()<tfprob else 0\n",
    "        xs.append(x)\n",
    "        y=1 if action == 0 else 0\n",
    "        ys.append(y)\n",
    "        \n",
    "        observation,reward,done,_ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            xs,hs,dlogps,drs,ys = [],[],[],[],[]\n",
    "            \n",
    "            discounted_epr = discount_rewards(drs)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            \n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "            \n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                \n",
    "                print 'Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size)\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print \"Task solved in\",episode_number,'episodes!'\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
